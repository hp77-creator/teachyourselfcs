## Lesson 1

[Lesson link in book](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb)

[Video Lesson](https://course.fast.ai/videos/?lesson=1)

Jeremy empahsises it enough that you don't need 
- Lot of data
- Lot of math
- Lot of GPUs

to get started in the field of deep learning. 

Then he goes on to tell about the history and the difference in fast.ai course and other deep learning courses around the corner.

Thing to note is that in fast.ai we don't use a basic-to-advanced approach but just like how you learn sport i.e first you try the game then you gradually go into details and this is how his course is structured.

Here are my answers to the questionaire for the first chapter:

- Q1. Do you need these for deep learning?

    - Lots of math **F**
    - Lots of data **F**
    - Lots of expensive computers **F**
    - A PhD **F**

- Q2. Are neural networks a recent invention? What year was the mathematical model of an artificial neuron developed?

    - No, Neural networks first came into existence when marvin minsky wrote a book about perceptrons but he did write that it will not be easy 


- Q2. Name five areas where deep learning is now the best in the world.
    
Ans.

    - Image Classification
    - Natural Language Processing
    - 


- Q4. ML Jargon - What do we call:
    
    - the functional form of a model = architecture
    - the weights = parameters
    - the results = predictions
    - the measure of performance = metric
    - the dependent variable = targets

- Q5. What is a positive feedback loop?

    - It's a situation where the result of a model's prediction affect its training and this results in a loop.

    - (Correct)
    A situation in which the more model is used, the more biased data becomes, making the model even more biased, and so forth. "Related issue is that of using data as a proxy for what you are after. In all cases, that data that you have is a proxy for some value that you truly care about and the difference between proxy and actual value often ends up being significant."


